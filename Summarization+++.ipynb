{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMBJej61KvHtvFZHmH3ie6G",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShaliniAnandaPhD/ShaliniAnandaPhD/blob/main/Summarization%2B%2B%2B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tika"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sH5a05cxz3CP",
        "outputId": "9dc4ca7b-da47-473d-ceaa-3c68f0829796"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tika\n",
            "  Downloading tika-2.6.0.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tika) (67.7.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from tika) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->tika) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->tika) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->tika) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->tika) (2023.7.22)\n",
            "Building wheels for collected packages: tika\n",
            "  Building wheel for tika (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tika: filename=tika-2.6.0-py3-none-any.whl size=32621 sha256=dd5eb927bb286f4dcaff5a937698b5a245e000c46d01fee96260efbbf87a4df0\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/71/c7/b757709531121b1700cffda5b6b0d4aad095fb507ec84316d0\n",
            "Successfully built tika\n",
            "Installing collected packages: tika\n",
            "Successfully installed tika-2.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "\n",
        "def download_pdf(url, file_path):\n",
        "  response = urllib.request.urlopen(url)\n",
        "  pdf_content = response.read()\n",
        "\n",
        "  with open(file_path, \"wb\") as f:\n",
        "    f.write(pdf_content)"
      ],
      "metadata": {
        "id": "8pL9T4Furiqy"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tika import parser\n",
        "import spacy\n",
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "def parse_pdf_content(file_path):\n",
        "  parsed_pdf = parser.from_file(file_path)\n",
        "  return parsed_pdf['content'].strip()\n",
        "\n",
        "def chunk_text(text, chunk_size=200, overlap=50):\n",
        "  tokens = text.split()\n",
        "  for i in range(0, len(tokens), chunk_size-overlap):\n",
        "    yield tokens[i:i+chunk_size]\n",
        "\n",
        "def get_text_vectors(text, chunk_size, overlap):\n",
        "  chunks = chunk_text(text, chunk_size, overlap)\n",
        "  return [nlp(\" \".join(chunk)) for chunk in chunks]\n",
        "\n",
        "def combine_semantic_chunks(vector_chunks, text_chunks):\n",
        "  paragraphs = [text_chunks[0]]\n",
        "\n",
        "  for i in range(1, len(vector_chunks)):\n",
        "    similarity = cosine(vector_chunks[i-1], vector_chunks[i])\n",
        "    if similarity < 0.8:\n",
        "      paragraphs.append(text_chunks[i])\n",
        "    else:\n",
        "      paragraphs[-1].extend(text_chunks[i])\n",
        "\n",
        "  return paragraphs"
      ],
      "metadata": {
        "id": "JqVBWMZ5rmX4"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "def score_sentences(sentences):\n",
        "  freqs = {}\n",
        "  for sent in sentences:\n",
        "    for word in word_tokenize(sent):\n",
        "      if word not in stopwords.words('english'):\n",
        "        freqs[word] = freqs.get(word, 0) + 1\n",
        "\n",
        "  max_freq = max(freqs.values())\n",
        "  for word in freqs:\n",
        "    freqs[word] /= max_freq\n",
        "\n",
        "  sent_scores = {sent:0 for sent in sentences}\n",
        "  for sent in sentences:\n",
        "    for word in word_tokenize(sent):\n",
        "      if word in freqs:\n",
        "        sent_scores[sent] += freqs[word]\n",
        "\n",
        "  return sent_scores\n",
        "\n",
        "def summarize(text, threshold=0.8):\n",
        "  sentences = sent_tokenize(text)\n",
        "  sent_scores = score_sentences(sentences)\n",
        "\n",
        "  summary = []\n",
        "  for sent, score in sent_scores.items():\n",
        "    if score > threshold:\n",
        "      summary.append(sent)\n",
        "\n",
        "  return \" \".join(summary)"
      ],
      "metadata": {
        "id": "Y6-RQL4ErsXy"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import requests\n",
        "from tika import parser\n",
        "import spacy\n",
        "from scipy.spatial.distance import cosine\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "# Global Variables\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# PDF Downloader\n",
        "def download_pdf(url, file_path):\n",
        "    response = requests.get(url)\n",
        "    content = response.content\n",
        "\n",
        "    with open(file_path, 'wb') as f:\n",
        "        f.write(content)\n",
        "\n",
        "# PDF Parsing\n",
        "def parse_pdf_content(file_path):\n",
        "    parsed = parser.from_file(file_path)\n",
        "    return parsed['content']\n",
        "\n",
        "def chunk_pdf_text(text, chunk_size, overlap):\n",
        "    tokens = text.split()\n",
        "\n",
        "    for i in range(0, len(tokens), chunk_size - overlap):\n",
        "        yield \" \".join(tokens[i:i + chunk_size])\n",
        "\n",
        "def vectorize_chunks(chunks):\n",
        "    return [nlp(chunk) for chunk in chunks]\n",
        "\n",
        "# Semantic Grouping\n",
        "def group_semantic_chunks(vector_chunks, text_chunks):\n",
        "    paragraphs = [text_chunks[0]]\n",
        "\n",
        "    for i in range(1, len(vector_chunks)):\n",
        "        similarity = 1 - cosine(vector_chunks[i-1].vector, vector_chunks[i].vector)\n",
        "\n",
        "        if similarity < 0.8:\n",
        "            paragraphs.append(text_chunks[i])\n",
        "\n",
        "        else:\n",
        "            paragraphs[-1] += \" \" + text_chunks[i]\n",
        "\n",
        "    return paragraphs\n",
        "\n",
        "# Summarization\n",
        "def score_sentences(sentences):\n",
        "    # Score logic\n",
        "    # TODO: Implement the score logic\n",
        "    pass\n",
        "\n",
        "def summarize(text, threshold):\n",
        "    # Summarize logic\n",
        "    # TODO: Implement the summarize logic\n",
        "    return text\n",
        "\n",
        "# Main Function\n",
        "def summarize_pdf(url, file):\n",
        "    # Download\n",
        "    download_pdf(url, file)\n",
        "\n",
        "    # Parse\n",
        "    text = parse_pdf_content(file)\n",
        "\n",
        "    # Detect Semantic Chunks\n",
        "    chunks = list(chunk_pdf_text(text, 200, 50))\n",
        "    vectors = vectorize_chunks(chunks)\n",
        "\n",
        "    # Group\n",
        "    paragraphs = group_semantic_chunks(vectors, chunks)\n",
        "\n",
        "    # Summarize\n",
        "    summaries = []\n",
        "\n",
        "    for para in paragraphs:\n",
        "        summary = summarize(para, 0.8)\n",
        "        summaries.append(summary)\n",
        "\n",
        "    return summaries\n",
        "\n",
        "# Driver\n",
        "url = 'https://www.uky.edu/Projects/Chemcomics/'\n",
        "summaries = summarize_pdf(url, 'sample.pdf')\n",
        "print(summaries)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPxOM82evBCY",
        "outputId": "261e0e9d-8eb3-40b6-ee3f-45785274f369"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-09-12 14:12:02,191 [MainThread  ] [INFO ]  Retrieving http://search.maven.org/remotecontent?filepath=org/apache/tika/tika-server-standard/2.6.0/tika-server-standard-2.6.0.jar to /tmp/tika-server.jar.\n",
            "INFO:tika.tika:Retrieving http://search.maven.org/remotecontent?filepath=org/apache/tika/tika-server-standard/2.6.0/tika-server-standard-2.6.0.jar to /tmp/tika-server.jar.\n",
            "2023-09-12 14:12:02,696 [MainThread  ] [INFO ]  Retrieving http://search.maven.org/remotecontent?filepath=org/apache/tika/tika-server-standard/2.6.0/tika-server-standard-2.6.0.jar.md5 to /tmp/tika-server.jar.md5.\n",
            "INFO:tika.tika:Retrieving http://search.maven.org/remotecontent?filepath=org/apache/tika/tika-server-standard/2.6.0/tika-server-standard-2.6.0.jar.md5 to /tmp/tika-server.jar.md5.\n",
            "2023-09-12 14:12:03,143 [MainThread  ] [WARNI]  Failed to see startup log message; retrying...\n",
            "WARNING:tika.tika:Failed to see startup log message; retrying...\n",
            "2023-09-12 14:12:08,153 [MainThread  ] [WARNI]  Failed to see startup log message; retrying...\n",
            "WARNING:tika.tika:Failed to see startup log message; retrying...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"The Comic Book Periodic Table of the Elements Welcome to the Periodic Table of Comic Books. Click on an element to see a list of comic book pages involvingthat element. Click on a thumbnail on the list to see a full comic bookpage. For technical information about an element, follow the link to Mark Winter's WebElements. We recommend that you start with oxygen to see some of our best stuff. There's something for everyone here! Over 6,000,000 Hits! John P. Selegue F. James Holler Department of Chemistry, Universityof Kentucky Lexington,KY 40506-0055 Version 2.012 This document and associatedimages ©1996-2012, F. James Holler and John P.Selegue. All rights reserved..\"]\n"
          ]
        }
      ]
    }
  ]
}